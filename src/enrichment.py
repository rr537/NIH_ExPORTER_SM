import pandas as pd
import logging
from typing import List, Tuple, Dict, Optional
from flashtext import KeywordProcessor
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from collections import Counter

def enrich_with_keyword_metrics(
    df: pd.DataFrame,
    config: Dict,
    treatments: List[str],
    diseases: List[str],
    logger: logging.Logger,
) -> pd.DataFrame:
    """
    Enrich DataFrame with keyword counts and matched disease/treatment terms.
    Adds: 'total count', 'total unique count', 'flagged'
    """
    text_cols = config.get("text_columns", [])
    available_cols = [col for col in text_cols if col in df.columns]

    if not available_cols:
        logger.warning(" No valid text columns found in DataFrame for enrichment.")
        return df

    # ðŸ”— Combine text from selected columns
    df["combined_text"] = df[available_cols].apply(
        lambda row: " | ".join(str(x).lower() for x in row if pd.notnull(x)),
        axis=1
    )

    # âš¡ Build FlashText processor
    keyword_processor = KeywordProcessor()
    keyword_processor.add_keywords_from_list([kw.lower() for kw in treatments + diseases])

    # ðŸ§µ Batching logic
    def chunk_series(series: pd.Series, chunk_size: int) -> List[pd.Series]:
        return [series[i:i + chunk_size] for i in range(0, len(series), chunk_size)]

    max_workers = config.get("workers", 4)
    chunk_size = max(1, len(df) // max_workers)
    chunks = chunk_series(df["combined_text"], chunk_size)

    # ðŸ§ª Batch processing
    def process_batch(batch: pd.Series) -> Tuple[List[int], List[int], List[List[str]]]:
        total, unique, flagged = [], [], []
        for text in batch:
            keywords = keyword_processor.extract_keywords(text)
            total.append(len(keywords))
            unique.append(len(set(keywords)))
            flagged.append(list(dict.fromkeys(keywords)))
        return total, unique, flagged

    # ðŸš€ Parallel execution
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="flash") as executor:
        results = list(executor.map(process_batch, chunks))

    # ðŸ“¦ Flatten results
    total_flat = [count for batch in results for count in batch[0]]
    unique_flat = [count for batch in results for count in batch[1]]
    flagged_flat = [kw_list for batch in results for kw_list in batch[2]]

    # ðŸ“Š Create enrichment summary
    enrichment_summary = {
    "total_rows_processed": len(df),
    "text_columns_used": available_cols,
    "max_workers": max_workers,
    "chunk_size": chunk_size,
    "keyword_pool_size" : len(set(treatments + diseases)),
    "treatment_pool_size": len(set(treatments)),
    "disease_pool_size": len(set(diseases)),
    "total_keyword_hits": sum(total_flat),
    "avg_hits_per_row": round(np.mean(total_flat), 2),
    "avg_unique_per_row": round(np.mean(unique_flat), 2),
    "rows_with_hits": sum(1 for c in total_flat if c > 0),
    "rows_with_hits_pct" : round(sum(1 for c in total_flat if c > 0) / len(df) * 100, 2),
    "rows_without_hits" : sum(1 for count in total_flat if count == 0),
    "rows_without_hits_pct" : round(sum(1 for count in total_flat if count == 0) / len(df) * 100, 2),
    "rows_flagged": sum(1 for flagged in flagged_flat if flagged),
    "top_flagged_terms": Counter([kw for row in flagged_flat for kw in row]).most_common(10)
}
    
    # ðŸ§¼ Assign new columns
    df["total count"] = pd.Series(total_flat, index=df.index)
    df["total unique count"] = pd.Series(unique_flat, index=df.index)
    df["flagged"] = pd.Series(flagged_flat, index=df.index)

    df.drop(columns="combined_text", inplace=True)
    logger.info(f" Keyword enrichment done: {df.shape[0]:,} rows tagged.")

    return df, enrichment_summary
